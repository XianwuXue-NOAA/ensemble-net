#!/usr/bin/env python3
#
# Copyright (c) 2017-18 Jonathan Weyn <jweyn@uw.edu>
#
# See the file LICENSE for your rights.
#

"""
Trains and tests an ensemble selection model using predictors generated by ens_sel_batch_process.py. Implements an
'online learning' scheme whereby chunks of the data are loaded dynamically and training occurs on these individual
chunks. Uses Keras fit_generator() method to do so.
Options for iterating over hyper-parameters.
This script is currently useless because an HDF lock error occurs when trying to fit the next model in the loop. Keras
seems to have an issue with terminating child processes of fit_generator.
"""

from ensemble_net.util import save_model, SGDLearningRateTracker, BatchHistory
from ensemble_net.ensemble_selection import preprocessing, verify
from ensemble_net.ensemble_selection.model import EnsembleSelector, DataGenerator
import numpy as np
import time
import xarray as xr
import os
import random
from shutil import copyfile
from keras.optimizers import SGD
from keras.callbacks import TerminateOnNaN, History
import keras.backend as K


#%% User parameters

# Paths to important files
root_data_dir = '%s/Data/ensemble-net' % os.environ['WORKDIR']
predictor_file = '%s/predictors_201504-201603_28N40N100W78W_x4_no_c.nc' % root_data_dir
model_file = '%s/selector_ncar_MSLP' % root_data_dir
convolved = False

# Copy file to scratch space
copy_file_to_scratch = True

# Optionally predict for only a subset of variables. Must use integer index as a list, or 'all'
variables = [-1]

# Predict with model spatial fields only, and no observational errors as inputs
model_fields_only = False

# Neural network configuration and options
batch_size = 6  # in model init dates
scaler_fit_size = 100
epochs = 1
impute_missing = True
scale_targets = False
val = 'random'
val_size = 46
# Use multiple GPUs
n_gpu = 1

# Neural network configurations
layers_1 = [
    ['Dense', (1024,), {
        'activation': 'relu',
        'input_shape': ()
    }],
    ('Dropout', (0.25,), {}),
    ['Dense', (), {
        'activation': 'linear'
    }]
]
layers_2 = [
    ['Dense', (1024,), {
        'activation': 'relu',
        'input_shape': ()
    }],
    ('Dropout', (0.25,), {}),
    ['Dense', (1024,), {
        'activation': 'relu'
    }],
    ('Dropout', (0.25,), {}),
    ['Dense', (), {
        'activation': 'linear'
    }]
]

# Seed the random validation set generator
random.seed(0)

# Save history at each batch or each epoch
save_batch_history = False

# Iterate over configurations or hyper-parameters
cv_parameters = {
    'lr': [3e-4, 1e-4, 3e-5, 1e-5, 3e-6],
    'structure': [layers_1, layers_2]
}


#%% End user configuration

# Parameter checks
if variables == 'all' or variables is None:
    ens_sel = {}
else:
    if type(variables) is not list and type(variables) is not tuple:
        try:
            variables = int(variables)
            variables = [variables]
        except (TypeError, ValueError):
            raise TypeError("'variables' must be a list of integers or 'all'")
    else:
        try:
            variables = [int(v) for v in variables]
        except (TypeError, ValueError):
            raise TypeError("indices in 'variables' must be integer types")
    ens_sel = {'obs_var': variables}


#%% Copy file; do the initial loading and assessing

# Copy the file to scratch, if requested, and available
try:
    job_id = os.environ['SLURM_JOB_ID']
except KeyError:
    copy_file_to_scratch = False
if copy_file_to_scratch:
    predictor_file_name = predictor_file.split('/')[-1]
    scratch_file = '/scratch/%s/%s/%s' % (os.environ['USER'], os.environ['SLURM_JOB_ID'], predictor_file_name)
    print('Copying predictor file to scratch space...')
    copyfile(predictor_file, scratch_file)
    predictor_file = scratch_file


# Load a Dataset with the predictors
print('Opening predictor dataset %s...' % predictor_file)
predictor_ds = xr.open_dataset(predictor_file, mask_and_scale=True)
num_dates = predictor_ds.dims['init_date']
num_members = predictor_ds.dims['member']
num_stations = predictor_ds.dims['station']

# Select the observation variables
predictor_ds = predictor_ds.sel(**ens_sel)
num_variables = predictor_ds.dims['obs_var']


#%% Get indices for the training and validation sets

if val == 'first':
    val_set = list(range(0, val_size))
    train_set = list(range(val_size, num_dates))
elif val == 'last':
    val_set = list(range(num_dates - val_size, num_dates))
    train_set = list(range(0, num_dates - val_size))
elif val == 'random':
    train_set = list(range(num_dates))
    val_set = []
    for j in range(val_size):
        i = random.choice(train_set)
        val_set.append(i)
        train_set.remove(i)
    val_set.sort()
else:
    raise ValueError("'val' must be 'first', 'last', or 'random'")


#%% Create an EnsembleSelector and Generator. The selector and generator are intertwined for scaling and imputing.

print('Building an EnsembleSelector model...')
selector = EnsembleSelector(impute_missing=impute_missing, scale_targets=scale_targets)

# Make a DataGenerator for training
generator = DataGenerator(selector, predictor_ds.isel(init_date=train_set), batch_size,
                          convolved=convolved, model_fields_only=model_fields_only)

# Make a DataGenerator for validation
val_generator = DataGenerator(selector, predictor_ds.isel(init_date=val_set), batch_size,
                              convolved=convolved, model_fields_only=model_fields_only)

# Initialize the model's Imputer and Scaler with a larger set of data
print('Fitting the EnsembleSelector Imputer and Scaler...')
fit_set = train_set[:scaler_fit_size]
predictors, targets = generator.generate_data(fit_set, scale_and_impute=False)
input_shape = predictors.shape[1:]
num_outputs = targets.shape[1]
conv_shape = generator.get_spatial_shape()
selector.init_fit(predictors, targets)
predictors = None
targets = None

# Load the validation set, which will now also be scaled
print('Processing validation set...')
p_val, t_val = val_generator.generate_data([])


#%% Build and train the ensemble selection model, iterating over parameters

for l_index, layers in enumerate(cv_parameters['structure']):
    for learning_rate in cv_parameters['lr']:
        layers[0][-1]['input_shape'] = input_shape
        layers[-1][1] = (num_outputs,)
        print(layers)
        sgd = SGD(lr=learning_rate)
        selector.build_model(layers=layers, gpus=n_gpu, loss='mse', optimizer=sgd, metrics=['mae'])

        # Train and evaluate the model
        print('Training the EnsembleSelector model...')
        start_time = time.time()
        if save_batch_history:
            history = BatchHistory()
        else:
            history = History()
        selector.fit_generator(generator, epochs=epochs, verbose=1, validation_data=(p_val, t_val),
                               use_multiprocessing=True,
                               callbacks=[TerminateOnNaN(), SGDLearningRateTracker(), history])

        end_time = time.time()

        # Use model.evaluate() because p_val and t_val are already scaled
        score = selector.model.evaluate(p_val, t_val, verbose=0)
        print("\nTrain time -- %s seconds --" % (end_time - start_time))
        print('Val loss:', score[0])
        print('Val mean absolute error:', score[1])

        # Save the model, if requested
        if model_file is not None:
            model_file += '_struct_%d_lr_%0.1E' % (l_index, learning_rate)
            print('Saving model to disk... (%s)' % model_file)
            save_model(selector, model_file, history=history)

        K.clear_session()

        # Reset the generators... seems to cause HDF errors in multiprocessing
        generator.ds.close()
        generator = None
        val_generator.ds.close()
        val_generator = None
        selector.model = None
        predictor_ds.close()
        predictor_ds = None
        predictor_ds = xr.open_dataset(predictor_file, mask_and_scale=True)
        generator = DataGenerator(selector, predictor_ds.isel(init_date=train_set), batch_size,
                                  convolved=convolved, model_fields_only=model_fields_only)
        val_generator = DataGenerator(selector, predictor_ds.isel(init_date=val_set), batch_size,
                                      convolved=convolved, model_fields_only=model_fields_only)
